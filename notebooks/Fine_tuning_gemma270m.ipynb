{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "ec58871a-67cc-4488-985d-ebccf040b16a",
      "metadata": {
        "id": "ec58871a-67cc-4488-985d-ebccf040b16a"
      },
      "source": [
        "# Fine Tuning the gemma-270m model"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "1f67df9e-b3bf-4296-ae4b-719027fbc001",
      "metadata": {
        "id": "1f67df9e-b3bf-4296-ae4b-719027fbc001"
      },
      "source": [
        "Neste notebook vamos realizar uma demonstração de um simples fine tuning com o modelo gemma-270m.\n",
        "## Motivação\n",
        "Este modelo é extremamente pequeno e eficiente para o seu tamanho, o que reduz drasticamente os gastos computacionais para a realização de um fine tuning. Cogitei realizar o fine tuning dentro do ambiente da Bedrock, porém como todo o projeto principal já utilizou o Bedrock para diversas tarefas, optei por realizar o fine tuning de outra forma, a fim de mostrar um espectro maior de habilidades. Além disso, pessoalmente já queria tentar realizar o fine tuning desde modelo desde o seu lançamento, há alguns meses.  "
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Se estiver no Google Colab tira o comentario abaixo depois da primeira execução\n",
        "!pip install unsloth datasets trl"
      ],
      "metadata": {
        "id": "pDrYAsV_qWM2"
      },
      "id": "pDrYAsV_qWM2",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "31969a18-6e34-4c0b-a0ab-41b1189bc93f",
      "metadata": {
        "id": "31969a18-6e34-4c0b-a0ab-41b1189bc93f"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import pandas as pd\n",
        "from datasets import load_dataset\n",
        "from unsloth import FastModel\n",
        "import json"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "a271b4eb-08e7-46f3-a98c-0b8d64f5afaa",
      "metadata": {
        "id": "a271b4eb-08e7-46f3-a98c-0b8d64f5afaa"
      },
      "outputs": [],
      "source": [
        "max_seq_len = 2048"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "72ed6f28-405a-49ec-a534-6518b6af80fb",
      "metadata": {
        "id": "72ed6f28-405a-49ec-a534-6518b6af80fb"
      },
      "outputs": [],
      "source": [
        "# https://colab.research.google.com/github/unslothai/notebooks/blob/main/nb/Gemma3_(270M).ipynb#scrollTo=-Xbb0cuLzwgf\n",
        "model, tokenizer = FastModel.from_pretrained(\n",
        "    model_name = \"unsloth/gemma-3-270m-it\",\n",
        "    max_seq_length = max_seq_len,\n",
        "    load_in_4bit = False,\n",
        "    load_in_8bit = False,\n",
        "    full_finetuning = False,\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "f944ac2a-561d-4550-bf90-82c54cd2c804",
      "metadata": {
        "id": "f944ac2a-561d-4550-bf90-82c54cd2c804"
      },
      "outputs": [],
      "source": [
        "model = FastModel.get_peft_model(\n",
        "    model,\n",
        "    r = 128, # Choose any number > 0 ! Suggested 8, 16, 32, 64, 128\n",
        "    target_modules = [\"q_proj\", \"k_proj\", \"v_proj\", \"o_proj\",\n",
        "                      \"gate_proj\", \"up_proj\", \"down_proj\",],\n",
        "    lora_alpha = 128,\n",
        "    lora_dropout = 0, # Supports any, but = 0 is optimized\n",
        "    bias = \"none\",    # Supports any, but = \"none\" is optimized\n",
        "    use_gradient_checkpointing = \"unsloth\", # True or \"unsloth\" for very long context\n",
        "    random_state = 3407,\n",
        "    use_rslora = False,  # We support rank stabilized LoRA\n",
        "    loftq_config = None, # And LoftQ\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "hWh3YxjxNzZw"
      },
      "id": "hWh3YxjxNzZw",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Para realizar o fine tuning, é necessário primeiramente transformar os dados do dataset utilizado em um formato em que o modelo possa aprender algo, cada modelo possui um template para uso e por isso vamos importar o template específico do gemma-3."
      ],
      "metadata": {
        "id": "SmqiqoXuKwRC"
      },
      "id": "SmqiqoXuKwRC"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "97ad4126-dc95-4daa-a286-e74a53af153e",
      "metadata": {
        "id": "97ad4126-dc95-4daa-a286-e74a53af153e"
      },
      "outputs": [],
      "source": [
        "from unsloth.chat_templates import get_chat_template\n",
        "tokenizer = get_chat_template(\n",
        "    tokenizer,\n",
        "    chat_template = \"gemma3\",\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Cada exemplo deve manter um papdrão de exemplo de conversa, a função abaixo realizará esta tarefa e será aplicada via map()."
      ],
      "metadata": {
        "id": "3vGsmIHtLGHb"
      },
      "id": "3vGsmIHtLGHb"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "b4deae0a-84db-4cbe-94d5-7512b1cc8163",
      "metadata": {
        "id": "b4deae0a-84db-4cbe-94d5-7512b1cc8163"
      },
      "outputs": [],
      "source": [
        "def processar_chatml(exemplo):\n",
        "    context = exemplo['context']\n",
        "    prompt = f\"Context: {context}\\nQuestion: {exemplo['question']}\"\n",
        "    completion = f\"Answer: {exemplo['answers']['text']}\"\n",
        "    return {\"conversations\": [\n",
        "        {\"role\": \"user\", \"content\": prompt},\n",
        "        {\"role\": \"assistant\", \"content\": completion}\n",
        "    ]}"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "As conversas padronizadas são então aplicadas utilizando a função apply_chat_template."
      ],
      "metadata": {
        "id": "TqtUZReGLyem"
      },
      "id": "TqtUZReGLyem"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "6f91a737-5326-4279-add4-dfbc20956e40",
      "metadata": {
        "id": "6f91a737-5326-4279-add4-dfbc20956e40"
      },
      "outputs": [],
      "source": [
        "def formatar_prompts(exemplos):\n",
        "   convos = exemplos[\"conversations\"]\n",
        "   texts = [tokenizer.apply_chat_template(convo, tokenize = False, add_generation_prompt = False).removeprefix('<bos>') for convo in convos]\n",
        "   return { \"text\" : texts, }\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "O dataset utilizado, 'Stanford Question Answering Dataset (SQuAD)', possui diversas perguntas e resposta, sendo utilizado neste fine tuning para melhorar a capacidade do modelo em responder perguntas trivia. Naturalmente, pode-se utilizar diversos tipos de dataset para melhorar as capacidades do modelo em um tópico específico. Testei diversos datasets antes de encontrar este, mas devido a complexidade dos datasets e a proposta do desafio de aplicar um fine tunin simples, optei por utilizar este mais generalista para otimizar o uso do tempo em outras tarefas.\n",
        "\n",
        "As funções apresentadas são aplicadas via map() nos dados do dataset e então separadas em dataset de treinamento e dataset de teste."
      ],
      "metadata": {
        "id": "0kcCZTCeMTLm"
      },
      "id": "0kcCZTCeMTLm"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "a68402cb-c0ee-41f3-a2c7-5b298dd70914",
      "metadata": {
        "id": "a68402cb-c0ee-41f3-a2c7-5b298dd70914"
      },
      "outputs": [],
      "source": [
        "\n",
        "dataset = load_dataset(\"rajpurkar/squad\")\n",
        "dataset = dataset.map(processar_chatml)\n",
        "dataset = dataset.map(formatar_prompts, batched = True)\n",
        "print(len(dataset['train']))\n",
        "dataset_misturado = dataset['train'].shuffle(seed=42)\n",
        "train_dataset = dataset_misturado.select(range(70000))\n",
        "test_dataset = dataset_misturado.select(range(70000, len(dataset['train'])))\n",
        "\n",
        "print(f\"Training set size: {len(train_dataset)}\")\n",
        "print(f\"Test set size: {len(test_dataset)}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Antes de iniciar o treinamento, vamos verificar uma resposta do modelo:"
      ],
      "metadata": {
        "id": "Tb7YiEb_O6EG"
      },
      "id": "Tb7YiEb_O6EG"
    },
    {
      "cell_type": "code",
      "source": [
        "messages = [\n",
        "    {'role': 'system','content':dataset['conversations'][10][0]['content']},\n",
        "    {\"role\" : 'user', 'content' : dataset['conversations'][10][1]['content']}\n",
        "]\n",
        "\n",
        "print(test_dataset['conversations'][10][0]['content'])\n",
        "print(test_dataset['conversations'][10][1]['content'])\n",
        "text = tokenizer.apply_chat_template(\n",
        "    messages,\n",
        "    tokenize = False,\n",
        "    add_generation_prompt = True, # Must add for generation\n",
        ").removeprefix('<bos>')\n",
        "\n",
        "print(\"Model answer:\")\n",
        "from transformers import TextStreamer\n",
        "_ = model.generate(\n",
        "    **tokenizer(text, return_tensors = \"pt\").to(\"cuda\"),\n",
        "    max_new_tokens = 250,\n",
        "    temperature = 1, top_p = 0.95, top_k = 64,\n",
        "    streamer = TextStreamer(tokenizer, skip_prompt = True),\n",
        ")\n",
        "\n",
        "pre_model = model"
      ],
      "metadata": {
        "id": "xvVdm-sYOA55"
      },
      "id": "xvVdm-sYOA55",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Agora é possível iniciar o processo de treinamento, onde colocamos o dataset de treinamento e o de verificação dos resultados, bem como alguns hiperparâmertos importantes para o treinamento. Para o leitor, os hiperparâmetros que mais modifiquei foram: 'warmup_steps', 'max_steps', 'num_train_epochs', 'weight_decay' e 'lr_scheduler_type'.\n"
      ],
      "metadata": {
        "id": "9nJfHn05NDeP"
      },
      "id": "9nJfHn05NDeP"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "804a8788-1587-458d-bce8-9127da0e4f06",
      "metadata": {
        "id": "804a8788-1587-458d-bce8-9127da0e4f06"
      },
      "outputs": [],
      "source": [
        "from trl import SFTTrainer, SFTConfig\n",
        "trainer = SFTTrainer(\n",
        "    model = model,\n",
        "    tokenizer = tokenizer,\n",
        "    train_dataset = train_dataset,\n",
        "    eval_dataset = test_dataset, # Can set up evaluation!\n",
        "    args = SFTConfig(\n",
        "        dataset_text_field = \"text\",\n",
        "        per_device_train_batch_size = 8,\n",
        "        gradient_accumulation_steps = 1, # Use GA to mimic batch size!\n",
        "        warmup_steps = 5,\n",
        "        #num_train_epochs = 1, # Set this for 1 full training run.\n",
        "        max_steps = 100,\n",
        "        learning_rate = 5e-5, # Reduce to 2e-5 for long training runs\n",
        "        logging_steps = 10,\n",
        "        optim = \"adamw_8bit\",\n",
        "        weight_decay = 0.01,\n",
        "        lr_scheduler_type = \"linear\",\n",
        "        seed = 3407,\n",
        "        output_dir=\"outputs\",\n",
        "        report_to = \"none\", # Use this for WandB etc\n",
        "    ),\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "c0deb30f-60be-4a61-bb71-e5665cebf766",
      "metadata": {
        "id": "c0deb30f-60be-4a61-bb71-e5665cebf766"
      },
      "outputs": [],
      "source": [
        "from unsloth.chat_templates import train_on_responses_only\n",
        "trainer = train_on_responses_only(\n",
        "    trainer,\n",
        "    instruction_part = \"<start_of_turn>user\\n\",\n",
        "    response_part = \"<start_of_turn>model\\n\",\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "tokenizer.decode(trainer.train_dataset[100][\"input_ids\"])"
      ],
      "metadata": {
        "id": "Hujyuszasixg"
      },
      "id": "Hujyuszasixg",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "tokenizer.decode([tokenizer.pad_token_id if x == -100 else x for x in trainer.train_dataset[100][\"labels\"]]).replace(tokenizer.pad_token, \" \").replace(\"<end_of_turn>\", \"\")"
      ],
      "metadata": {
        "id": "YGCXBPU4snRe"
      },
      "id": "YGCXBPU4snRe",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Agora podemos iniciar o treinamento, nos meus testes o loss caiu de ~3.3 a ~1.8, mostrando que o modelo realmente aprendeu alguma coisa."
      ],
      "metadata": {
        "id": "Wq1FDWrmNjPq"
      },
      "id": "Wq1FDWrmNjPq"
    },
    {
      "cell_type": "code",
      "source": [
        "trainer_stats = trainer.train()"
      ],
      "metadata": {
        "id": "qyA3NOxata51"
      },
      "id": "qyA3NOxata51",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Podemos então obter uma resposta do modelo com fine tuning:"
      ],
      "metadata": {
        "id": "UQbzd_jlNsxG"
      },
      "id": "UQbzd_jlNsxG"
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"Modelo com fine tuning:\")\n",
        "indice_do_item = 12 # Troque para obter novas perguntas e respostas\n",
        "messages = [\n",
        "    {'role': 'system','content':test_dataset['conversations'][indice_do_item][0]['content']},\n",
        "    {\"role\" : 'user', 'content' : test_dataset['conversations'][indice_do_item][1]['content']}\n",
        "]\n",
        "print(test_dataset['conversations'][indice_do_item][0]['content'])\n",
        "print(test_dataset['conversations'][indice_do_item][1]['content'])\n",
        "text = tokenizer.apply_chat_template(\n",
        "    messages,\n",
        "    tokenize=False,\n",
        "    add_generation_prompt=True\n",
        ").removeprefix('<bos>')\n",
        "print(\"Model answer:\")\n",
        "from transformers import TextStreamer\n",
        "_ = model.generate(\n",
        "    **tokenizer(text, return_tensors=\"pt\").to(\"cuda\"),\n",
        "    max_new_tokens=250,\n",
        "    temperature=1, top_p=0.95, top_k=64,\n",
        "    streamer=TextStreamer(tokenizer, skip_prompt=True)\n",
        ")"
      ],
      "metadata": {
        "id": "ZBN2U-xZtjhD"
      },
      "id": "ZBN2U-xZtjhD",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "E o modelo sem fine tuning."
      ],
      "metadata": {
        "id": "rUFZ41qtbl0B"
      },
      "id": "rUFZ41qtbl0B"
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"Modelo sem fine tuning:\")\n",
        "indice_do_item = 12 # Troque para obter novas perguntas e respostas\n",
        "messages = [\n",
        "    {'role': 'system','content':test_dataset['conversations'][indice_do_item][0]['content']},\n",
        "    {\"role\" : 'user', 'content' : test_dataset['conversations'][indice_do_item][1]['content']}\n",
        "]\n",
        "print(test_dataset['conversations'][indice_do_item][0]['content'])\n",
        "print(test_dataset['conversations'][indice_do_item][1]['content'])\n",
        "text = tokenizer.apply_chat_template(\n",
        "    messages,\n",
        "    tokenize=False,\n",
        "    add_generation_prompt=True\n",
        ").removeprefix('<bos>')\n",
        "print(\"Model answer:\")\n",
        "from transformers import TextStreamer\n",
        "_ = pre_model.generate(\n",
        "    **tokenizer(text, return_tensors=\"pt\").to(\"cuda\"),\n",
        "    max_new_tokens=250,\n",
        "    temperature=1, top_p=0.95, top_k=64,\n",
        "    streamer=TextStreamer(tokenizer, skip_prompt=True)\n",
        ")"
      ],
      "metadata": {
        "id": "Q4iA4nH1ZKUs"
      },
      "id": "Q4iA4nH1ZKUs",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Como pode ser visto, o modelo com fine tuning apresenta"
      ],
      "metadata": {
        "id": "hjDGBN1Zh3pR"
      },
      "id": "hjDGBN1Zh3pR"
    },
    {
      "cell_type": "code",
      "source": [
        "model.save_pretrained(\"gemma-3\")  # Local saving\n",
        "tokenizer.save_pretrained(\"gemma-3\")"
      ],
      "metadata": {
        "id": "3qYiQNYWuX--"
      },
      "id": "3qYiQNYWuX--",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "noqnmtjyvVN-"
      },
      "id": "noqnmtjyvVN-",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "VsCvuYb9wMfS"
      },
      "id": "VsCvuYb9wMfS",
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.11"
    },
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 5
}